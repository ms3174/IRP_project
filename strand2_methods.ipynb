{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #; sns.set_theme() # For heat map\n",
    "from scipy.optimize import minimize # For automatic minimisation of the negative\n",
    "# likelihood of the position\n",
    "from ipynb.fs.full.class_virt_sensor import virt_sensor_net2\n",
    "from ipynb.fs.full.class_ship import ship_fleet\n",
    "from ipynb.fs.full.class_black_sea import black_sea_obj\n",
    "import numpy.random as rnd\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.path as mpltPath # to check if the point is on the border\n",
    "from scipy.optimize import NonlinearConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Need to define the black sea object before reading the methods in this notebook\n",
    "\n",
    "black_sea_coords = np.load('blackSea_polygon_coords.npy')\n",
    "global black_sea\n",
    "black_sea = black_sea_obj(black_sea_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENT VERSION: The function to be optimised w.r.t. the sensor placement\n",
    "# (or rather minimised) to be the mean of the average value of all ship lane \n",
    "# prediction errors where each ship lane is repeated num_rep times to account \n",
    "# for random errors in the automated gradient descent w.r.t. the likelihood \n",
    "# maximisation procedure.\n",
    "\n",
    "# NEXT VERSION: Add a function that approximates the gradient of the above \n",
    "# function by moving each sensor individual in each direction slightly, repeating\n",
    "# the entire mean average measuring process, and then using this insight to\n",
    "# approximate the gradient of the entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIKELIHOOD (virtual sensors)\n",
    "\n",
    "def nlhd(xy, network, ship_idx): # Negative likelihood at point xy for ship \"ship_idx\"\n",
    "    lh_val = - network.get_net_likelihood(ship_idx, xy) # Negative network likelihood\n",
    "    return lh_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN GRADIENT DESCENT METHOD (virtual sensors): To be optimised (minimised) MSE loss\n",
    "\n",
    "def MSE_loss(sensor_coords): # MSE loss minimising sensor placement\n",
    "    count = 0\n",
    "    time = 0\n",
    "    network = virt_sensor_net2(sensor_coords.reshape(m,2))\n",
    "    for i in range(fleet.size):\n",
    "        \n",
    "        while(count < num_rep):\n",
    "            \n",
    "            prev_est = rnd.uniform(0, .2, size = (2,))\n",
    "            while(time < time_stop):\n",
    "            \n",
    "                fleet.update_fleet_positions(time) # Update the position of all ships\n",
    "                if(time == 0):\n",
    "                    lane_actual = fleet.coords[i]\n",
    "                else:\n",
    "                    lane_actual = np.vstack((lane_actual, fleet.coords[i])) # Save position for lane MSE\n",
    "                network.def_gaussian_spheres(fleet, err_mean, err_stdev) # Update\n",
    "                # network Gaussian spheres for new fleet position\n",
    "\n",
    "                xy_0 = prev_est\n",
    "                coord_pred_at_time = minimize(nlhd, xy_0, args = (network, i),\n",
    "                                              method='nelder-mead',\n",
    "                                              options={'xatol': 1e-4}).x\n",
    "                if(time == 0):\n",
    "                    lane_pred = coord_pred_at_time\n",
    "                else:\n",
    "                    lane_pred = np.vstack((lane_pred,coord_pred_at_time))\n",
    "                prev_est = coord_pred_at_time\n",
    "                time += 1 \n",
    "                \n",
    "            if(count == 0):\n",
    "                lane_i_error = np.mean(linalg.norm(lane_actual - lane_pred)**2)\n",
    "            else:\n",
    "                lane_i_error = np.vstack((lane_i_error, np.mean(linalg.norm(lane_actual - lane_pred)**2)))\n",
    "            count += 1\n",
    "            \n",
    "        if(i==0):\n",
    "            MSE_for_lane_i = np.mean(lane_i_error)\n",
    "        else:\n",
    "            MSE_for_lane_i = np.vstack((MSE_for_lane_i, np.mean(lane_i_error)))\n",
    "\n",
    "    avg_MSE_over_all_lanes = np.mean(MSE_for_lane_i)\n",
    "    \n",
    "    return avg_MSE_over_all_lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENT FOR GRADIENT DESCENT (virtual sensors): Pseudo gradient at the current position\n",
    "\n",
    "def pseudo_MSE_loss_gradient(sensor_coords): # Shifting individual sensors, and\n",
    "# repetition of measuring to define a gradient approximation in the point\n",
    "    \n",
    "    eps = .05 # Position change in EACH of the (x,y) directions\n",
    "    sensor_coords = sensor_coords.reshape(m, 2)\n",
    "    \n",
    "    pseudo_loss_gradient = []\n",
    "    for i in range(m):\n",
    "        x_i_eps_shift = np.zeros((m,2))\n",
    "        x_i_eps_shift[i, 0] += eps\n",
    "        sensor_coords_x_i_plus_eps = sensor_coords + x_i_eps_shift\n",
    "        sensor_coords_x_i_minus_eps = sensor_coords - x_i_eps_shift\n",
    "        loss_gradient_x_i = (MSE_loss(sensor_coords_x_i_plus_eps) - MSE_loss(sensor_coords_x_i_minus_eps))/eps\n",
    "        \n",
    "        y_i_eps_shift = np.zeros((m,2))\n",
    "        y_i_eps_shift[i, 1] += eps\n",
    "        sensor_coords_y_i_plus_eps = sensor_coords + y_i_eps_shift\n",
    "        sensor_coords_y_i_minus_eps = sensor_coords - y_i_eps_shift\n",
    "        loss_gradient_y_i = (MSE_loss(sensor_coords_y_i_plus_eps) - MSE_loss(sensor_coords_y_i_minus_eps))/eps\n",
    "        \n",
    "        pseudo_loss_gradient.append(loss_gradient_x_i) # Output needs to be a gradient vector in the form (dx_1, dy_1, dx_2, dy_2,...)\n",
    "        pseudo_loss_gradient.append(loss_gradient_y_i)\n",
    "    \n",
    "    return np.asarray(pseudo_loss_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENT FOR GRADIENT DESCENT (virtual sensors): Will be used to define the contraint given to the SLSQP method\n",
    "def within_borders(sensor_coords):\n",
    "    # Using a path over the black sea coordinates defined in \"black_sea.border_points\"\n",
    "    # to check if ALL points (written as matrix) are still within the borders\n",
    "    \n",
    "    # PROBLEM: Global definition of black_sea before reading the method does NOT suffice to define the black sea object\n",
    "    \n",
    "    m = int(len(sensor_coords.reshape(-1,1))/2) # Need it for if-condition below\n",
    "    sensor_coords = sensor_coords.reshape(m,2)    \n",
    "    path = mpltPath.Path(black_sea.border_points)\n",
    "    \n",
    "    if (m == 1):# ATTENTION: For multiple points we need \"path.contains_points\"\n",
    "    # and then it only works for multiple points but not for a single one\n",
    "        tf_value = path.contains_point(sensor_coords.reshape(-1))\n",
    "        if tf_value:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        tf_value = path.contains_points(sensor_coords)    \n",
    "        if min(tf_value): # tf_value is a vector of True/False; we only allow all True\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that chooses optimal number of sensors based on AIC criterion\n",
    "\n",
    "\n",
    "# ToDo: Need method that places sensors randomly in the allowed area; IDEA:\n",
    "# using acceptance-rejection, the method draws uniform random variables until\n",
    "# it has the number of needed sensors\n",
    "\n",
    "def random_sensor_placement(number_sensors):\n",
    "    n = number_sensors\n",
    "    sensor_coords = []\n",
    "    while (len(sensor_coords) < n):\n",
    "        trial_sensor = rnd.uniform(0, 1, size = 2)\n",
    "        if (within_borders(trial_sensor) == 1):\n",
    "            sensor_coords.append(trial_sensor)\n",
    "    return np.array(sensor_coords).reshape(-1)\n",
    "\n",
    "\n",
    "def calc_model_loss(mse_loss, complexity, lambd):\n",
    "    '''\n",
    "    # AIC criterion approach\n",
    "    k = 2*complexity # Number of parameter estimates (as the total number of xy coordinates)\n",
    "    rating = 2*k - np.log(mse_loss) #PROBLEM: MSE loss is <<1 since we do not work\n",
    "    # with likelihood functions (which is assumed by Akaike); FIX: Employ a\n",
    "    # Lasso/Ridge like approach by penalising complexity by additive loss\n",
    "    '''\n",
    "    # Lasso/Ridge approach\n",
    "    model_loss = mse_loss + lambd*complexity\n",
    "    return model_loss\n",
    "\n",
    "def get_optimal_sensor_number(max_num):\n",
    "    log_book = [[],[],[]] # this log book should hold the information of the procedure\n",
    "    # where the first component holds the initial sensor placements, the second\n",
    "    # the \"optimal\" sensor coordinates after optimisation, and the last should\n",
    "    # hold the rating of the model under optimality; this is continuously written\n",
    "    # over changing m = 1,...,max_num\n",
    "    global m\n",
    "    m = 1\n",
    "    best_number = m # We will change the optimal number of sensors iteratively\n",
    "    # from one iteration to the next ONLY if the new number has a lower model loss\n",
    "    best_model_loss = 1e5 # Big initial loss to start overwriting\n",
    "    # FOR ANALYSIS: Also save the worst number of sensors\n",
    "    worst_number = 1\n",
    "    worst_model_loss = 0 \n",
    "    while (m < max_num):\n",
    "        coords_init = random_sensor_placement(m)\n",
    "        bnds = (((0,1),) * 2*m) # Need to define 2m bounds for each element of the variable vector\n",
    "        # NOTE: 'ftol' is the termination tolerance w.r.t. the gradient at the points (since we want to minimise)\n",
    "        res = minimize(MSE_loss, coords_init, method = 'SLSQP', jac = pseudo_MSE_loss_gradient,\n",
    "               bounds = bnds, constraints = nlc, options={'ftol': 1e-10, 'maxiter': 200, 'disp': True})\n",
    "        coords_opt = res.x\n",
    "        mse_loss = res.fun\n",
    "        model_loss = calc_model_loss(mse_loss, m, .001) # TODO: Decide on optimal lambda value\n",
    "        # through cross-validation ?; for now, it is only fixed\n",
    "        if (model_loss < best_model_loss):\n",
    "            best_model_loss = model_loss\n",
    "            best_number = m\n",
    "            \n",
    "        # FOR ANALYSIS:\n",
    "        if (model_loss > worst_model_loss):\n",
    "            worst_number = m\n",
    "            worst_model_loss = model_loss\n",
    "        \n",
    "        m += 1\n",
    "        log_book[0].append(coords_init)\n",
    "        log_book[1].append(coords_opt)\n",
    "        log_book[2].append(model_loss)\n",
    "    \n",
    "    return best_number, worst_number, log_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR OUTPUT\n",
    "# For prediction output: Re-do the repetitive likelihood prediction for the \n",
    "# optimal network coordinates and plot the mean predictions\n",
    "    \n",
    "def MSE_loss_plot(sensor_coords): # MSE loss minimising sensor placement\n",
    "    network = virt_sensor_net2(sensor_coords.reshape(m,2))\n",
    "    for i in range(fleet.size):\n",
    "        count = 0\n",
    "        while(count < num_rep):\n",
    "            time = 0\n",
    "            # PROBLEM with random initial value: Too much of a wild card\n",
    "            '''\n",
    "            prev_est = rnd.uniform(10,40, size = (2,))\n",
    "            '''\n",
    "            # CURRENT VERSION: Set initial start point for negative likelihood\n",
    "            # minimisation by hand\n",
    "            \n",
    "            prev_est = np.array([.2, .2])\n",
    "            \n",
    "            while(time < time_stop):\n",
    "            \n",
    "                fleet.update_fleet_positions(time) # Update the position of all ships\n",
    "                if(time == 0):\n",
    "                    lane_actual = fleet.coords[i]\n",
    "                else:\n",
    "                    lane_actual = np.vstack((lane_actual, fleet.coords[i])) # Save position for lane MSE\n",
    "                network.def_gaussian_spheres(fleet, err_mean, err_stdev) # Update\n",
    "                # network Gaussian spheres for new fleet position\n",
    "\n",
    "                xy_0 = prev_est\n",
    "                # PROBLEM: 'Nelder-Mead' approach does not allow for bounds w.r.t.\n",
    "                # the position\n",
    "                \n",
    "                coord_pred_at_time = minimize(nlhd, xy_0, args = (network, i),\n",
    "                                              method='nelder-mead',\n",
    "                                              options={'xatol': 1e-8}).x\n",
    "                \n",
    "                # TRIAL: Use 'SLSQP' algorithm where we bound the values onto the\n",
    "                # space of interest\n",
    "                '''\n",
    "                bnds = ((0, 50),(0, 50))\n",
    "                coord_pred_at_time = minimize(nlhd, xy_0, args = (network, i),\n",
    "                                              method='SLSQP',\n",
    "                                              bounds = bnds).x\n",
    "                '''\n",
    "                if(time == 0):\n",
    "                    lane_pred = coord_pred_at_time\n",
    "                else:\n",
    "                    lane_pred = np.vstack((lane_pred,coord_pred_at_time))\n",
    "                prev_est = coord_pred_at_time\n",
    "                time += 1 \n",
    "                \n",
    "            if(count == 0):\n",
    "                lane_i_error = np.mean(linalg.norm(lane_actual - lane_pred)**2)\n",
    "                # OUTPUT CHANGE IS HERE:\n",
    "                # For illustration: Save lane prediction now and add others later\n",
    "                # to get easy mean estimate afterwards\n",
    "                test_mean_lane_pred = lane_pred\n",
    "            else:\n",
    "                lane_i_error = np.vstack((lane_i_error, np.mean(linalg.norm(lane_actual - lane_pred)**2)))\n",
    "                # OUTPUT CHANGE IS HERE:\n",
    "                test_mean_lane_pred += lane_pred\n",
    "            count += 1\n",
    "            \n",
    "        if(i==0):\n",
    "            MSE_for_lane_i = np.mean(lane_i_error)\n",
    "            # OUTPUT CHANGE IS HERE:\n",
    "            # Save the mean lane predictions as list of arrays\n",
    "            test_lane_pred = [num_rep**(-1) * test_mean_lane_pred]\n",
    "            test_actual_lanes = [lane_actual]\n",
    "        else:\n",
    "            MSE_for_lane_i = np.vstack((MSE_for_lane_i, np.mean(lane_i_error)))\n",
    "            # OUTPUT CHANGE IS HERE:\n",
    "            # For test: Save the mean lane predictions as list of arrays\n",
    "            test_lane_pred.append(num_rep**(-1) * test_mean_lane_pred)\n",
    "            test_actual_lanes.append(lane_actual)\n",
    "\n",
    "    avg_MSE_over_all_lanes = np.mean(MSE_for_lane_i)\n",
    "    \n",
    "    print(\"The average MSE over all lanes and repetitions is: \" + str(avg_MSE_over_all_lanes))\n",
    "    # ONLY for test: Print the ships, sensors and their predictions\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(network.coords[:,0], network.coords[:,1], 'b^')\n",
    "    for i in range(fleet.size):\n",
    "        fig = plt.gcf()\n",
    "        ax = fig.gca()\n",
    "        ax.plot(test_actual_lanes[i][:,0], test_actual_lanes[i][:,1], 'ro')\n",
    "        ax.plot(test_lane_pred[i][:,0], test_lane_pred[i][:,1], 'go')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
